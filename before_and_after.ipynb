{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.49s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "/home/rickzhou/miniconda3/envs/function_calling/lib/python3.11/site-packages/transformers/generation/utils.py:2097: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'find_max_value', 'description': 'Find the maximum value in an array', 'parameters': {'type': 'object', 'properties': {'array': {'type': 'array', 'items': {'type': 'number'}, 'description': 'The array to search in'}}, 'required': ['array']}}}, {'type': 'function', 'function': {'name': 'search_restaurants', 'description': 'Search for restaurants based on location and cuisine', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The location of the restaurant'}, 'cuisine': {'type': 'string', 'description': 'The cuisine type of the restaurant'}, 'price_range': {'type': 'string', 'description': 'The price range of the restaurant'}}, 'required': ['location']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
      "<tool_call>\n",
      "{tool_call}\n",
      "</tool_call><|im_end|>\n",
      "<|im_start|>user\n",
      "Hi, I have an array of numbers and I need to find the maximum value. The array is [5, 7, 2, 9, 1, 6].<|im_end|>\n",
      "\n",
      "<|im_start|>system\n",
      "The maximum value in the array is 9.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"/home/rickzhou/function_calling/models/starcoder2-7b\"\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/home/rickzhou/function_calling/models/starcoder2-7b\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_flash_attention_2=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "prompt = [\n",
    "    {\n",
    "        \"from\": \"system\",\n",
    "        \"value\": \"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'find_max_value', 'description': 'Find the maximum value in an array', 'parameters': {'type': 'object', 'properties': {'array': {'type': 'array', 'items': {'type': 'number'}, 'description': 'The array to search in'}}, 'required': ['array']}}}, {'type': 'function', 'function': {'name': 'search_restaurants', 'description': 'Search for restaurants based on location and cuisine', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The location of the restaurant'}, 'cuisine': {'type': 'string', 'description': 'The cuisine type of the restaurant'}, 'price_range': {'type': 'string', 'description': 'The price range of the restaurant'}}, 'required': ['location']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\\n<tool_call>\\n{tool_call}\\n</tool_call>\",\n",
    "    },\n",
    "    {\n",
    "        \"from\": \"human\",\n",
    "        \"value\": \"Hi, I have an array of numbers and I need to find the maximum value. The array is [5, 7, 2, 9, 1, 6].\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def format_conversation(example):\n",
    "    \"\"\"Format conversation using ChatML template.\"\"\"\n",
    "    try:\n",
    "        conversation = \"\"\n",
    "        for message in example:\n",
    "            if (\n",
    "                not isinstance(message, dict)\n",
    "                or \"from\" not in message\n",
    "                or \"value\" not in message\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            role = message[\"from\"]\n",
    "            if role == \"system\":\n",
    "                conversation += f\"<|im_start|>system\\n{message['value']}<|im_end|>\\n\"\n",
    "            elif role == \"human\":\n",
    "                conversation += f\"<|im_start|>user\\n{message['value']}<|im_end|>\\n\"\n",
    "            elif role == \"gpt\":\n",
    "                conversation += f\"<|im_start|>assistant\\n{message['value']}<|im_end|>\\n\"\n",
    "        return conversation\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    prompt_formatted = format_conversation(prompt)\n",
    "    print(prompt_formatted)\n",
    "\n",
    "    inputs = tokenizer(prompt_formatted, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=1024,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        stop_strings=[\"<|im_end|>\"],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    outputs = outputs[:, inputs[\"input_ids\"].shape[-1] :]\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'find_max_value', 'description': 'Find the maximum value in an array', 'parameters': {'type': 'object', 'properties': {'array': {'type': 'array', 'items': {'type': 'number'}, 'description': 'The array to search in'}}, 'required': ['array']}}}, {'type': 'function', 'function': {'name': 'search_restaurants', 'description': 'Search for restaurants based on location and cuisine', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The location of the restaurant'}, 'cuisine': {'type': 'string', 'description': 'The cuisine type of the restaurant'}, 'price_range': {'type': 'string', 'description': 'The price range of the restaurant'}}, 'required': ['location']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
      "<tool_call>\n",
      "{tool_call}\n",
      "</tool_call><|im_end|>\n",
      "<|im_start|>user\n",
      "Hi, I have an array of numbers and I need to find the maximum value. The array is [5, 7, 2, 9, 1, 6].<|im_end|>\n",
      "\n",
      "<|im_start|>assistant\n",
      "<tool_call>\n",
      "{'name': 'find_max_value', 'arguments': {'array': [5, 7, 2, 9, 1, 6]}}\n",
      "</tool_call><|im_end|>\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    model, \"/home/rickzhou/function_calling/sft/starcoder-function-calling\"\n",
    ")\n",
    "\n",
    "peft_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    prompt_formatted = format_conversation(prompt)\n",
    "    print(prompt_formatted)\n",
    "\n",
    "    inputs = tokenizer(prompt_formatted, return_tensors=\"pt\")\n",
    "    outputs = peft_model.generate(\n",
    "        **inputs,\n",
    "        max_length=1024,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        stop_strings=[\"<|im_end|>\"],\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    outputs = outputs[:, inputs[\"input_ids\"].shape[-1] :]\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "function_calling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
